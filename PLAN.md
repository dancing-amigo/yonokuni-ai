# 実装プラン（今後のタスク）

Yonokuni AI は基盤となるルール実装・Gym 環境・自己対戦/リプレイ基盤・MCTS を整備済みです。ここからはニューラルネットワークと学習ループを中心に、評価・運用まで段階的に拡張していきます。

## 1. ニューラルネットワークと推論基盤
- [ ] Small ResNet（例: 128ch × 10〜12 Block）で方策1792ロジット/価値1 を出力する PyTorch モデルを実装。
- [ ] 盤面テンソル＋補助ベクトルの入力エンコーダ（FiLM 等）を構築し、`state_to_torch` から直接 Tensor を取得できるよう統合。
- [ ] 推論ユーティリティ（デバイス管理、混合精度、バッチ推論、TorchScript/ONNX エクスポート）を整備。
- [ ] モデル重みの保存/読み込み・バージョニング API を提供。

## 2. MCTS × ネットワーク統合
- [ ] ニューラルネットをコールバックとする評価関数を実装し、`MCTSPolicy` で使用。
- [ ] 温度/探索回数/Dirichlet 設定を Hydra 等の設定システムで切り替え可能にする。
- [ ] バッチ推論（複数シミュレーションの評価をまとめる）を検討し、GPU 使用効率を向上。
- [ ] MCTS ロギング（探索深度分布、visit count、Q値ヒストグラム）を整備。

## 3. 学習パイプライン
- [ ] リプレイバッファをディスク永続化可能な形に拡張（シャーディング、メタ情報含む）。
- [ ] DataLoader 相当のサンプラを実装し、ミニバッチで `(board, aux, policy_target, value_target)` を供給。
- [ ] トレーニングループを構築（Hydra/argparse 構成、AdamW + CosineDecay、損失 `CE(policy)+MSE(value)+λL2`、mixed precision、勾配クリップ）。
- [ ] ログ・可視化（TensorBoard/W&B）とチェックポイント（モデル/オプティマイザ/バッファ）保存・再開機構を追加。

## 4. 自己対戦スケーリングとデータ品質
- [ ] 自己対戦ワーカーをマルチプロセス化し、同時に複数環境を走らせるマネージャを構築。
- [ ] 温度スケジュール（序盤 τ=1 → 終盤 τ→0）、探索回数プリセット（学習用/早指し）を設定化。
- [ ] 収集データの検証スクリプト（NaN、勝率/手数の異常、色バランス、value drift）を追加。
- [ ] データ拡張（D4 + 色循環）の適用状況を検証するユニットテスト・統計レポートを整備。

## 5. 評価とゲーティング
- [ ] ルールベース Bot / ランダム / 過去モデルとの評価マッチを自動化。
- [ ] 勝率・平均手数・中央勝利率・死に駒発生ターンなどを集計しダッシュボード化。
- [ ] 新旧モデルのゲーティング（勝率 >55% で昇格）パイプラインを実装し、結果ログを永続化。
- [ ] 重要局面のリプレイ生成（MCTS visit 分布、推論ロジット、最終盤面）を自動エクスポート。

## 6. 推論・デプロイ準備
- [ ] 本番推論向けランタイム（軽量 MCTS/方策単独、バッチ推論対応）をモジュール化。
- [ ] 対局サーバや UI との連携 API（REST/WebSocket/CLI）を整理。
- [ ] モデルリリースフロー（バージョニング、リリースノート、ロールバック手順）と監視項目（レイテンシ/勝率ドリフト/MCTS 探索深度）を策定。

## 7. インフラ・ナレッジ
- [ ] Docker イメージ化し、自己対戦・学習・評価・推論を統一環境で再現可能にする。
- [ ] スケジューラ（Kubernetes/Airflow 等）によるジョブ管理と GPU/CPU リソース配分ポリシーを整備。
- [ ] バックアップ戦略（モデル・リプレイバッファ・ログ）と復旧手順を文書化。
- [ ] 仕様/設定/運用手順のドキュメント、オンボーディングガイド、定期レビュー体制を整備。

この更新版プランは、「モデル実装 → MCTS 統合 → 自己対戦スケール → 学習/評価 → 運用」の順で開発を進める指針を示します。*** End Patch
